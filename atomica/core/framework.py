# -*- coding: utf-8 -*-
"""
Atomica project-framework file.
Contains all information describing the context of a project.
This includes a description of the Markov chain network underlying project dynamics.
"""
import sciris.core as sc
from .structure import CoreProjectStructure, get_quantity_type_list
from .structure_settings import FrameworkSettings as FS, DataSettings as DS, TableTemplate
from .system import SystemSettings as SS, apply_to_all_methods, log_usage, logger, AtomicaException
from .workbook_export import make_instructions, write_workbook, make_framework_file
from .workbook_import import read_workbook
from .parser_function import parse_function
from .excel import read_tables, AtomicaSpreadsheet
import openpyxl
import pandas as pd

class ProjectFramework(object):
    """ The object that defines the transition-network structure of models generated by a project. """

    def __init__(self):

        # These will be lists of dicts that map to the columns displayed in the spreadsheet
        # self.comps = None
        # self.characs = None
        # self.interpops = None
        # self.pars = None
        # self.transitions = None
        #
        # self.databook_pages = None
        # self.cascades = None
        # self.plots = None

        self.sheets = sc.odict()

    def to_spreadsheet(self):
        raise NotImplementedError()

    @staticmethod
    def from_spreadsheet(spreadsheet):
        self = ProjectFramework()

        if isinstance(spreadsheet,str):
            spreadsheet = AtomicaSpreadsheet(spreadsheet)

        workbook = openpyxl.load_workbook(spreadsheet.get_file(),read_only=True,data_only=True) # Load in read-write mode so that we can correctly dump the file

        for worksheet in workbook.worksheets:
            tables = read_tables(worksheet) # Read the tables
            self.sheets[worksheet.title] = list()
            for table in tables:
                # Get a dataframe
                df = pd.DataFrame.from_records(table).applymap(lambda x: x.value)
                df.columns = df.iloc[0]
                df = df[1:]
                self.sheets[worksheet.title].append(df)

            # For convenience, if there is only one table (which is the case for most of the sheets) then
            # don't store it as a list. So there will only be a list of DataFrames if there was more than
            # one table on the page (e.g. for cascades)
            if len(self.sheets[worksheet.title]) == 1:
                self.sheets[worksheet.title] = self.sheets[worksheet.title][0]

        self.validate()

        return self

    # The primary data storage in the Framework are DataFrames with the contents of the Excel file
    # The convenience methods below enable easy access of frequency used contents without having
    # to store them separately or going via the DataFrames every time
    @property
    def comps(self):
        # Shortcut to Compartments sheet
        return self.sheets['Compartments']

    @comps.setter
    def comps(self, value):
        assert isinstance(value,pd.DataFrame)
        self.sheets['Compartments'] = value

    def get_comp(self,comp_name):
        return self.comps.loc[self.comps['Code Name'] == comp_name]

    @property
    def characs(self):
        # Shortcut to Characteristics sheet
        return self.sheets['Characteristics']

    @characs.setter
    def characs(self, value):
        assert isinstance(value,pd.DataFrame)
        self.sheets['Characteristics'] = value

    def get_charac(self,comp_name):
        return self.characs.loc[self.characs['Code Name'] == comp_name]

    @property
    def pars(self):
        # Shortcut to Parameters sheet
        return self.sheets['Parameters']

    @pars.setter
    def pars(self, value):
        assert isinstance(value,pd.DataFrame)
        self.sheets['Parameters'] = value

    def get_par(self,comp_name):
        return self.pars.loc[self.pars['Code Name'] == comp_name]

    def get_variable(self,code_name):
        # This function will return either a Comp, a Charac, or a Par
        match, item_type = self.get_comp(code_name), FS.KEY_COMPARTMENT
        if match.empty:
            match, item_type = self.get_charac(code_name), FS.KEY_CHARACTERISTIC
        if match.empty:
            match, item_type = self.get_par(code_name), FS.KEY_PARAMETER
        if match.empty:
            raise AtomicaException('Variable "%s" not found in Framework' % (code_name))

        return match, item_type

    def process_transitions(self):
        # Parse the dataframe associated with the transition sheet into an edge-list representation
        # with a dict where the key is a parameter name and the value is a list of (from,to) tuples
        #
        # Expects a sheet called 'Transitions' to be present and correctly filled out
        t = self.sheets['Transitions'].copy() # Copy the dataframe on this sheet
        assert isinstance(t,pd.DataFrame) # This will be a list if there was more than one item present

        self.transitions = {x:list() for x in list(self.pars['Code Name'])}
        comp_names = set(self.comps['Code Name'])

        for _,from_row in t.iterrows(): # For each row in the transition matrix
            from_row.dropna(inplace=True)
            from_comp = from_row[0]
            assert from_comp in comp_names
            from_row = from_row[1:]
            for to_comp,par_name in from_row.iteritems():
                assert par_name in self.transitions, 'Parameter %s appears in the transition matrix but not on the Parameters page' % (par_name)
                assert to_comp in comp_names
                self.transitions[par_name].append((from_comp,to_comp))

    def validate(self):
        # This function validates the content of Framework. There are two aspects to this
        # - Adding in any missing values using appropriate defaults
        # - Checking that the provided information is internally consistent

        # Check for required sheets
        for page in ['Databook Pages','Compartments','Parameters','Characteristics','Transitions']:
            assert page in self.sheets, 'Framework File missing required sheet "%s"' % (page)

        ### VALIDATE COMPARTMENTS
        required = ['Code Name','Display Name','Is Source', 'Is Sink','Is Junction','Databook Page']
        defaults = {
            'Is Sink':'n',
            'Is Source':'n',
            'Is Junction':'n',
            'Can Calibrate':'n',
            'Databook Order':None, # Default is for it to be randomly ordered if the Databook Page is not None
        }
        valid_content = {
            'Code Name':None,
            'Display Name':None,
            'Is Sink':{'y','n'},
            'Is Source':{'y','n'},
            'Is Junction':{'y','n'},
            'Can Calibrate':{'y','n'},
        }

        self.comps = sanitize_dataframe(self.comps, required, defaults, valid_content)

        # Default setup weight is 1 if in databook or 0 otherwise
        # This is a separate check because the default value depends on other columns
        if 'Setup Weight' not in self.comps:
            self.comps['Setup Weight'] = (~self.comps['Databook Page'].isnull()).astype(int)
        else:
            fill_ones = self.comps['Setup Weight'].isnull() & self.comps['Databook Page']
            self.comps['Setup Weight'][fill_ones] = 1
            self.comps['Setup Weight'].fillna(0, inplace=True)

        # VALIDATE THE COMPARTMENT SPECIFICATION
        for index,row in self.comps.iterrows():
            n_types = (row[['Is Sink','Is Source','Is Junction']]=='y').astype(int).sum() # This sums the number of Y's for each compartment
            assert n_types <= 1, 'Compartment "%s" can only be one of Sink, Source, or Junction' % row['Code Name']

            if (row['Setup Weight']>0) & (row['Is Source']=='y' or row['Is Sink']=='y'):
                raise AtomicaException('Compartment "%s" is a source or a sink, but has a nonzero setup weight' % row['Code Name'])

            if (row['Databook Page'] is not None) & (row['Is Source']=='y' or row['Is Sink']=='y'):
                raise AtomicaException('Compartment "%s" is a source or a sink, but has a Databook Page' % row['Code Name'])

            if (row['Databook Page'] is None) and (row['Databook Order'] is not None):
                logger.warning('Compartment "%s" has a databook order, but no databook page' % row['Code Name'])

        ### VALIDATE PARAMETERS

        required = ['Code Name','Display Name','Format','Databook Page']
        defaults = {
            'Default Value':None,
            'Minimum Value':None,
            'Maximum Value':None,
            'Function':None,
            'Databook Order':None,
            'Is Impact':'n',
            'Can Calibrate':'n',
        }
        valid_content = {
            'Code Name': None,
            'Display Name': None,
            'Is Impact':{'y','n'},
            'Can Calibrate':{'y','n'},
        }

        self.pars = sanitize_dataframe(self.pars, required, defaults, valid_content)

        # Parse the transitions matrix
        self.process_transitions()

        # Now validate each parameter
        for i,par in self.pars.iterrows():
            if self.transitions[par['Code Name']]: # If this parameter is associated with transitions

                # Units must be specified if this is a function parameter (in which case the databook does not specify the units)
                if (par['Function'] is not None) and (par['Format'] is None):
                    raise AtomicaException('Parameter %s has a custom function and is a transition parameter, so needs to have a format specified in the Framework' % par['Code Name'])

                from_comps = [x[0] for x in self.transitions[par['Code Name']]]
                to_comps = [x[1] for x in self.transitions[par['Code Name']]]

                # Avoid discussions about how to disaggregate parameters with multiple links from the same compartment.
                if len(from_comps) != len(set(from_comps)):
                    raise AtomicaException('Parameter "%s" cannot be associated with more than one transition from the same compartment' % par['Code Name'])

                n_special_outflow = 0
                for comp in from_comps:
                    comp_spec = self.get_comp(comp)
                    if comp_spec['Is Sink']=='y':
                        raise AtomicaException('Parameter "%s" has an outflow from Compartment "%s" which is a sink' % par['Code Name'],comp)
                    elif comp_spec['Is Source']=='y':
                        n_special_outflow += 1
                        assert par['format'] == FS.QUANTITY_TYPE_NUMBER, 'Parameter "%s" has an outflow from a source compartment, so it needs to be in "number" units' % par['Code Name']
                    elif comp_spec['Is Junction']=='y':
                        n_special_outflow += 1
                        assert par['format'] == FS.QUANTITY_TYPE_PROPORTION, 'Parameter "%s" has an outflow from a junction, so it must be in "proportion" units' % par['Code Name']

                    if (par['format'] == FS.QUANTITY_TYPE_PROPORTION) and (comp_spec['Is Junction']!='y'):
                        raise AtomicaException('"Parameter "%s" has units of "proportion" which means all of its outflows must be from junction compartments, which Compartment "%s" is not',par['Code Name'],comp)

                if n_special_outflow > 1:
                    raise AtomicaException('Parameter "%s" has an outflow more than one source compartment or junction, which prevents disaggregation from working correctly' % par['Code Name'])

                for comp in to_comps:
                    if self.get_comp(comp)['Is Source']=='y':
                        raise AtomicaException('Parameter "%s" has an inflow to Compartment "%s" which is a source' % par['Code Name'],comp)

        ### VALIDATE CHARACTERISTICS
        required = ['Code Name','Display Name','Databook Page']
        defaults = {
            'Components':None,
            'Denominator':None,
            'Default Value':None,
            'Function':None,
            'Databook Order':None,
            'Can Calibrate':'n',
        }
        valid_content = {
            'Code Name': None,
            'Display Name': None,
            'Can Calibrate':{'y','n'},
        }

        self.characs = sanitize_dataframe(self.characs, required, defaults, valid_content)

        if 'Setup Weight' not in self.characs:
            self.characs['Setup Weight'] = (~self.characs['Databook Page'].isnull()).astype(int)
        else:
            fill_ones = self.characs['Setup Weight'].isnull() & self.characs['Databook Page']
            self.characs['Setup Weight'][fill_ones] = 1
            self.characs['Setup Weight'].fillna(0, inplace=True)

        for i,spec in self.characs.iterrows():
            continue

    def get_allowed_units(self,code_name):
        # Given the name of a variable that matches either the full name or the code name of a Compartment, Characteristic, or Parameter
        # Return a list of allowed units
        item_type = self.get_spec_type(code_name)  # 'charac','par' etc.
        item_specs = self.get_spec(code_name)

        # State variables are in number amounts unless normalized.
        if item_type in [FS.KEY_COMPARTMENT, FS.KEY_CHARACTERISTIC]:
            if "denominator" in item_specs and item_specs["denominator"] is not None:
                allowed_units = [FS.QUANTITY_TYPE_FRACTION.title()]
            else:
                allowed_units = [FS.QUANTITY_TYPE_NUMBER.title()]

        # Modeller's choice for parameters
        elif item_type in [FS.KEY_PARAMETER] and "format" in item_specs and item_specs["format"] is not None:
            allowed_units = [item_specs["format"].title()]
        else:
            # User choice if a transfer or a transition parameter.
            if item_type in [FS.KEY_TRANSFER] or (FS.KEY_TRANSITIONS in item_specs and len(item_specs[FS.KEY_TRANSITIONS]) > 0):
                allowed_units = [FS.QUANTITY_TYPE_NUMBER.title(), FS.QUANTITY_TYPE_PROBABILITY.title()]
            # If not a transition, the format of this parameter is meaningless.
            else:
                allowed_units = [SS.DEFAULT_SYMBOL_INAPPLICABLE.title()]

        return allowed_units

    @staticmethod
    def create_template(path, num_comps=0, num_characs=0, num_pars=0, num_datapages=0, num_interpops=0):
        make_framework_file(path, datapages=num_datapages, comps=num_comps, characs=num_characs, interpops=num_interpops, pars=num_pars)
        return path

    def write_to_file(self, filename, data=None, instructions=None):
        """ Export a framework to file. """
        # TODO: modify write_workbook so it can write framework specs to an excel file???
        pass

    def read_from_file(self, filepath=None, overwrite=False):
        """ Import a framework from file. """
        if overwrite:
            print('Overwriting...')
            self.specs = sc.odict()
            self.semantics = sc.odict()
            self.filter = dict()
            self.init_specs()
        read_workbook(workbook_path=filepath, framework=self,
                      workbook_type=SS.STRUCTURE_KEY_FRAMEWORK)
        self.workbook_load_date = sc.today()
        self.modified = sc.today()


def sanitize_dataframe(df,required,defaults,valid_content):
    # Take in a DataFrame and sanitize it
    # INPUTS
    # - df : The DataFrame being sanitized
    # - required : A list of column names that *must* be present
    # - defaults : A dict/odict mapping column names to default values. If a column is not present, it will be initialized with this value. If entries in this column are None, they will be assigned this value
    #              The default value can be a lambda function
    # - valid_content : A dict mapping column names to valid content. If specified, all elements of the column must be members of the iterable (normally this would be a set)
    #                   If 'valid_content' is None, then instead it will be checked that all of the values are NOT null i.e. use valid_content=None to specify it cannot be empty

    # First check required columns are present
    for col in required:
        assert col in df, 'DataFrame did not contain the required column "%s"' % col

    # Then fill in default values
    for col, default_value in defaults.items():
        if col not in df:
            df[col] = default_value
        elif default_value is not None:
            df[col].fillna(default_value,inplace=True)

    # Finally check content
    for col, validation in valid_content.items():
        assert col in df, 'DataFrame does not contain column "%s" which was specified for validation' % (col)
        if validation is None:
            assert not df[col].isnull().any(), 'DataFrame column "%s" cannot contain any empty cells' % (col)
        else:
            validation = set(validation)
            assert set(df[col]).issubset(validation), 'DataFrame column "%s" can only contain the following values: %s' % (col,validation)
    return df